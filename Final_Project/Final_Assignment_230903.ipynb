{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f204cd6-2ff4-45a2-b435-2aa3803f9597",
   "metadata": {},
   "source": [
    "Fact Filter Final Assignment Samarth Sharma Bhardwaj 230903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40477917-9070-4a08-aa06-f4f303b98db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a678ab-ac08-4922-9bfc-7de3259b3ec0",
   "metadata": {},
   "source": [
    "Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbbafed3-ad47-45db-81d6-3376b552c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "fake_news = pd.read_csv('Fake.csv')\n",
    "true_news = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b982e0-5a6d-40cd-b830-1a44645ee93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n",
      "                                               title  \\\n",
      "0  As U.S. budget fight looms, Republicans flip t...   \n",
      "1  U.S. military to accept transgender recruits o...   \n",
      "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
      "3  FBI Russia probe helped by Australian diplomat...   \n",
      "4  Trump wants Postal Service to charge 'much mor...   \n",
      "\n",
      "                                                text       subject  \\\n",
      "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
      "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
      "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
      "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
      "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
      "\n",
      "                 date  \n",
      "0  December 31, 2017   \n",
      "1  December 29, 2017   \n",
      "2  December 31, 2017   \n",
      "3  December 30, 2017   \n",
      "4  December 29, 2017   \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23481 entries, 0 to 23480\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    23481 non-null  object\n",
      " 1   text     23481 non-null  object\n",
      " 2   subject  23481 non-null  object\n",
      " 3   date     23481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 733.9+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21417 entries, 0 to 21416\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    21417 non-null  object\n",
      " 1   text     21417 non-null  object\n",
      " 2   subject  21417 non-null  object\n",
      " 3   date     21417 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 669.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Explore datasets\n",
    "print(fake_news.head())\n",
    "print(true_news.head())\n",
    "print(fake_news.info())\n",
    "print(true_news.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8ee93-853a-40bb-bbf3-f1085dbf25fa",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9bef394-ea56-40c0-af6b-5ce1ba0b0595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Data Cleaning\n",
    "fake_news['label'] = 0\n",
    "true_news['label'] = 1\n",
    "\n",
    "# Combine the datasets\n",
    "data = pd.concat([fake_news, true_news], axis=0)\n",
    "data = data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aadcacf-c856-4bcd-abda-8713e2e04191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#2. Missing values handling\n",
    "print(data.isnull().sum())\n",
    "# If there are missing values, handle them accordingly, e.g., dropping or filling\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17b3ecdd-8dfd-4c19-ab9b-b9aac8cdb109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#3. Data Tokenization and Stop Word Removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenization and stop word removal\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word.lower() not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "761ab31b-9e67-44aa-b00a-4a4276fa6ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Samarth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85168c8c-192d-4ba6-8af2-fb75756bec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Data Vectorization using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(data['text'])\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077acd2-ad79-421c-b23d-cedfcf3864ff",
   "metadata": {},
   "source": [
    "Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73c8d807-4808-4922-8cbf-09b7820f4cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a85f1022-17dc-4135-ac3c-c6b6e8523cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14ebfea0-dd1b-4871-9cfd-8868f7c6c32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters grid for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 10, 20]},\n",
    "    'Gradient Boosting': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 1], 'max_depth': [3, 4, 5]}\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff0a607-95fa-48ff-8b2b-066172a698ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter tuning for Logistic Regression\n",
      "Logistic Regression tuned and evaluated.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "models = {'Logistic Regression': LogisticRegression()}\n",
    "# Perform grid search and evaluation for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Hyperparameter tuning for {model_name}\")\n",
    "    grid = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy')\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_model = grid.best_estimator_\n",
    "    best_models[model_name] = best_model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Store evaluation results\n",
    "    evaluation_results.append({\n",
    "        'Model': model_name,\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC-ROC': auc_roc\n",
    "    })\n",
    "    \n",
    "    print(f\"{model_name} tuned and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2409f8b-5cba-4c3c-8b00-f23cda752aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model Best Params  Accuracy  Precision    Recall  F1 Score  \\\n",
      "0  Logistic Regression  {'C': 100}  0.995768    0.99669  0.994338  0.995513   \n",
      "\n",
      "    AUC-ROC  \n",
      "0  0.995693  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
